# EvalPrompt

EvalPrompt is a modular, extensible library designed to evaluate LLM prompts with automatic metrics, model judges, and AB testing. The goal of the project is to offer a clean, scalable architecture that supports multiple model providers through a plugin system.

This document describes Phase 1 of the development, which focuses on establishing the core structure and interfaces of the project.

## Project Status

Current phase: Phase 1
State: Base architecture created. No functional evaluation pipeline implemented yet.

## Goals of Phase 1

Phase 1 provides the foundations of the entire project. It introduces the core building blocks that will support all future features:

1. Minimal package structure
2. Core data types for prompts, examples, and results
3. Abstract interfaces for providers, metrics, and judges
4. Basic registries for providers and metrics
5. Initial public API

With these elements in place, EvalPrompt has a stable skeleton ready for future development.

## Project Structure

```
evalprompt/
    __init__.py

    core/
        __init__.py
        evaluator.py
        runner.py
        types.py

    providers/
        __init__.py
        base.py
        registry.py

    metrics/
        __init__.py
        base.py
        registry.py

    judges/
        __init__.py
        base.py
```

This modular structure defines the main subsystems of EvalPrompt, even if many of the files are still empty placeholders.

## Core Types

The following core data structures are implemented in `core/types.py`:

### PromptDefinition

Represents a named, parameterised prompt template.

### EvalExample

Represents a single evaluation case with input data, optional expected output, and metadata.

### ModelOutput

Stores the output generated by a model for a given example.

### MetricResult

Represents the result of applying a metric to a prediction.

These are the foundational objects that will be used throughout the evaluation pipeline.

## Interfaces Implemented

### ModelProvider

Abstract interface for any LLM provider. All providers must implement a `generate` method.

### Metric

Abstract interface for metrics that compute a value from a prediction and an expected output.

### Judge

Abstract interface for judge components that can evaluate outputs individually or in pairs.

These interfaces guarantee extensibility and a clean separation of responsibilities.

## Registries Implemented

### ProviderRegistry

Holds registered model providers. Will be extended in Phase 2 with automatic plugin loading.

### MetricRegistry

Holds registered metric types. Allows dynamic extension without modifying the core package.

## Next Steps

The next phase will introduce a full plugin system for model providers using Python entry points. This will allow EvalPrompt to automatically detect providers installed as separate packages, such as `evalprompt-openai`.

Upcoming phases:

Phase 2: Plugin-based provider system
Phase 3: ExactMatch metric and the first end to end evaluation
Phase 4: Metric system expansion
Phase 5: Judge system and subjective evaluation
Phase 6: AB testing engine
Phase 7: Reporting modules
Phase 8: Advanced dataset support
Phase 9: Optimisation and optional integrations
Phase 10: Documentation and PyPI release
